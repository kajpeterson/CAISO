{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f4d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made new TempDir1\n",
      "made new TempDir2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/2qqrxy2d4vs58h_j13bl4_zc0000gn/T/ipykernel_70318/202624430.py:113: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(executable_path=chromedriver, options=options) #DeprecationWarning: use options instead of chrome_options // browser = webdriver.Chrome(executable_path=chromedriver, chrome_options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 04/01/2023\n",
      "downloading 04/02/2023\n",
      "downloading 04/03/2023\n",
      "downloading 04/04/2023\n",
      "downloading 04/05/2023\n",
      "downloading 04/06/2023\n",
      "downloading 04/07/2023\n",
      "downloading 04/08/2023\n",
      "downloading 04/09/2023\n",
      "downloading 04/10/2023\n",
      "downloading 04/11/2023\n",
      "downloading 04/12/2023\n",
      "downloading 04/13/2023\n",
      "downloading 04/14/2023\n",
      "downloading 04/15/2023\n",
      "downloading 04/16/2023\n",
      "downloading 04/17/2023\n",
      "downloading 04/18/2023\n",
      "downloading 04/19/2023\n",
      "downloading 04/20/2023\n",
      "downloading 04/21/2023\n",
      "downloading 04/22/2023\n",
      "downloading 04/23/2023\n",
      "downloading 04/24/2023\n",
      "downloading 04/25/2023\n",
      "downloading 04/26/2023\n",
      "downloading 04/27/2023\n",
      "downloading 04/28/2023\n",
      "downloading 04/29/2023\n",
      "downloading 04/30/2023\n",
      "30 files in all_filenames\n",
      "formatting data...\n",
      "formatting 04/13/2023\n",
      "autotrimming 04/13/2023 because it has 288 values.\n",
      "Fixed 04/13/2023. Now has 288 values.\n",
      "formatting 04/07/2023\n",
      "autotrimming 04/07/2023 because it has 288 values.\n",
      "Fixed 04/07/2023. Now has 288 values.\n",
      "formatting 04/06/2023\n",
      "autotrimming 04/06/2023 because it has 288 values.\n",
      "Fixed 04/06/2023. Now has 288 values.\n",
      "formatting 04/12/2023\n",
      "autotrimming 04/12/2023 because it has 288 values.\n",
      "Fixed 04/12/2023. Now has 288 values.\n",
      "formatting 04/04/2023\n",
      "autotrimming 04/04/2023 because it has 288 values.\n",
      "Fixed 04/04/2023. Now has 288 values.\n",
      "formatting 04/10/2023\n",
      "autotrimming 04/10/2023 because it has 288 values.\n",
      "Fixed 04/10/2023. Now has 288 values.\n",
      "formatting 04/11/2023\n",
      "autotrimming 04/11/2023 because it has 288 values.\n",
      "Fixed 04/11/2023. Now has 288 values.\n",
      "formatting 04/05/2023\n",
      "autotrimming 04/05/2023 because it has 288 values.\n",
      "Fixed 04/05/2023. Now has 288 values.\n",
      "formatting 04/29/2023\n",
      "autotrimming 04/29/2023 because it has 288 values.\n",
      "Fixed 04/29/2023. Now has 288 values.\n",
      "formatting 04/01/2023\n",
      "autotrimming 04/01/2023 because it has 288 values.\n",
      "Fixed 04/01/2023. Now has 288 values.\n",
      "formatting 04/15/2023\n",
      "autotrimming 04/15/2023 because it has 288 values.\n",
      "Fixed 04/15/2023. Now has 288 values.\n",
      "formatting 04/14/2023\n",
      "autotrimming 04/14/2023 because it has 288 values.\n",
      "Fixed 04/14/2023. Now has 288 values.\n",
      "formatting 04/28/2023\n",
      "autotrimming 04/28/2023 because it has 288 values.\n",
      "Fixed 04/28/2023. Now has 288 values.\n",
      "formatting 04/16/2023\n",
      "autotrimming 04/16/2023 because it has 288 values.\n",
      "Fixed 04/16/2023. Now has 288 values.\n",
      "formatting 04/02/2023\n",
      "autotrimming 04/02/2023 because it has 288 values.\n",
      "Fixed 04/02/2023. Now has 288 values.\n",
      "formatting 04/03/2023\n",
      "autotrimming 04/03/2023 because it has 288 values.\n",
      "Fixed 04/03/2023. Now has 288 values.\n",
      "formatting 04/17/2023\n",
      "autotrimming 04/17/2023 because it has 288 values.\n",
      "Fixed 04/17/2023. Now has 288 values.\n",
      "formatting 04/26/2023\n",
      "autotrimming 04/26/2023 because it has 288 values.\n",
      "Fixed 04/26/2023. Now has 288 values.\n",
      "formatting 04/27/2023\n",
      "autotrimming 04/27/2023 because it has 288 values.\n",
      "Fixed 04/27/2023. Now has 288 values.\n",
      "formatting 04/25/2023\n",
      "autotrimming 04/25/2023 because it has 288 values.\n",
      "Fixed 04/25/2023. Now has 288 values.\n",
      "formatting 04/19/2023\n",
      "autotrimming 04/19/2023 because it has 288 values.\n",
      "Fixed 04/19/2023. Now has 288 values.\n",
      "formatting 04/18/2023\n",
      "autotrimming 04/18/2023 because it has 288 values.\n",
      "Fixed 04/18/2023. Now has 288 values.\n",
      "formatting 04/30/2023\n",
      "autotrimming 04/30/2023 because it has 288 values.\n",
      "Fixed 04/30/2023. Now has 288 values.\n",
      "formatting 04/24/2023\n",
      "autotrimming 04/24/2023 because it has 288 values.\n",
      "Fixed 04/24/2023. Now has 288 values.\n",
      "formatting 04/08/2023\n",
      "autotrimming 04/08/2023 because it has 288 values.\n",
      "Fixed 04/08/2023. Now has 288 values.\n",
      "formatting 04/20/2023\n",
      "autotrimming 04/20/2023 because it has 288 values.\n",
      "Fixed 04/20/2023. Now has 288 values.\n",
      "formatting 04/21/2023\n",
      "autotrimming 04/21/2023 because it has 288 values.\n",
      "Fixed 04/21/2023. Now has 288 values.\n",
      "formatting 04/09/2023\n",
      "autotrimming 04/09/2023 because it has 288 values.\n",
      "Fixed 04/09/2023. Now has 288 values.\n",
      "formatting 04/23/2023\n",
      "autotrimming 04/23/2023 because it has 288 values.\n",
      "Fixed 04/23/2023. Now has 288 values.\n",
      "formatting 04/22/2023\n",
      "autotrimming 04/22/2023 because it has 288 values.\n",
      "Fixed 04/22/2023. Now has 288 values.\n",
      "concatenating...\n",
      "combined_csv\n",
      "saved combined csv\n",
      "adding datetime for sorting...\n",
      "sorting\n",
      "-------------------------\n",
      "dates queried: 30 days, 0:00:00\n",
      "dates returned: 30\n",
      "Given dates returned:\n",
      "  values expected: 8640\n",
      "  values returned: 8640\n",
      "dates with missing or extra data: []\n",
      "-------------------------\n",
      "saved final csv\n",
      "checking for duplicates...\n",
      "Value column used in Duplicate Check: Net demand\n",
      "duplicate_data_log: []\n",
      "76.84755396842957 seconds elapsed\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#CAISO TODAY'S OUTLOOK SCRAPER\n",
    "#=================================================================================\n",
    "#DOWNLOADS DAILY 5-MIN DATA FOR A SPECIFIED DATE RANGE AND DATASET FROM CAISO'S \"TODAY'S OUTLOOK\" PAGE.\n",
    "#SAVES TO path/CAISOcsv/Results, WHERE \"path\" IS THE DIRECTORY WHERE THIS FILE IS SAVED\n",
    "\n",
    "#The following libraries must be installed:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#INPUTS\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "url = 'https://www.caiso.com/TodaysOutlook/Pages/index.html'\n",
    "date_xpath = '//*[@id=\"section-demand-trend\"]/div[4]/div/div/div/nav/div[1]/input'\n",
    "dropdown_xpath = '/html/body/div[1]/div[3]/div[6]/div[4]/div/div/div/nav/div[3]/button'\n",
    "download_xpath = '/html/body/div[1]/div[3]/div[6]/div[4]/div/div/div/nav/div[3]/div/a'\n",
    "\n",
    "filename = 'netdemand_test2.csv'\n",
    "\n",
    "#date format: datetime.date(year,month,day)\n",
    "startdate = datetime.date(2023,4,1)\n",
    "enddate = datetime.date(2023,4,30)\n",
    "\n",
    "#Set headless to 'yes' to hide browser.\n",
    "headless = 'yes'\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "#Must have Chrome browser.\n",
    "#Must have corresponding version of chromedriver installed here (or change path below):\n",
    "chromedriver = '/usr/local/bin/chromedriver'\n",
    "\n",
    "#INSTRUCTIONS:\n",
    "#Update Inputs\n",
    "#Click 'Save'\n",
    "#Click 'Run' (or hit SHIFT-ENTER)\n",
    "\n",
    "#NOTE:\n",
    "#Some data cleanup will probably be required, for example due to errors in CAISO's data.\n",
    "#See error report in terminal for which dates need to be fixed.\n",
    "#Some common errors:\n",
    "    #Date has 289 values instead of 288.\n",
    "        #Sometimes CAISO's data for a given date includes an extra value at the end, corresponding to the following day's first value.\n",
    "        #This appears in the results file as 2 \"0:00\" values.\n",
    "        #See \"dates with missing or extra data\" list returned in terminal.\n",
    "        #Fix: The code should fix this itself, but if it doesn't, find the imposter value (probably a 0:00). If one of the 2 0:00 values matches the 0:00 value from the following date, delete that value.\n",
    "        #If it's unclear which value is wrong, download the date's CSV manually from Today's Outlook (or just hover over the chart). That CSV will still be in the original order, so the imposter 0:00 value will appear at the end.\n",
    "    #Duplication. If the code tries to download a date's data before it finishes loading, the previous date's data will be duplicated.\n",
    "        #See \"duplicate_data_log:\" list returned in terminal\n",
    "        #Fix: Download the duplicate date individually, or run code again for that range of dates. Replace duplicate data with newly downloaded data.\n",
    "\n",
    "#=================================================================================\n",
    "#SETUP DIRECTORIES AND CHROMEDRIVER\n",
    "\n",
    "#Make CAISOcsv & Results folders if they don't already exist\n",
    "#Clear and make temporary directories for downloads and intermediary transposed csv's\n",
    "#TempDir1 for downloads, TempDir2 for intermiediary transposed csv's, Results for final csv's:\n",
    "tic = time.time()\n",
    "date = startdate\n",
    "path = sys.path[0]\n",
    "CAISOcsv_path = f'{path}/CAISOcsv/'\n",
    "Results_path = f'{path}/CAISOcsv/Results/'\n",
    "TempDir1_path = f'{path}/CAISOcsv/TempDir1/'\n",
    "TempDir2_path = f'{path}/CAISOcsv/TempDir2/'\n",
    "combined_csv_path = f'{path}/CAISOcsv/TempDir2/combined_csv.csv'\n",
    "\n",
    "try:\n",
    "    os.mkdir(CAISOcsv_path)\n",
    "    print(\"made new CAISOcsv folder\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(Results_path)\n",
    "    print(\"made new Results folder\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(TempDir1_path)\n",
    "except FileExistsError:\n",
    "    try:\n",
    "        shutil.rmtree(TempDir1_path)\n",
    "        os.mkdir(TempDir1_path)\n",
    "        print(\"made new TempDir1\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" %(TempDir1_path, e.strerror))\n",
    "\n",
    "try:\n",
    "    os.mkdir(TempDir2_path)\n",
    "except FileExistsError:\n",
    "    try:\n",
    "        shutil.rmtree(TempDir2_path)\n",
    "        os.mkdir(TempDir2_path)\n",
    "        print(\"made new TempDir2\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" %(TempDir2_path, e.strerror))\n",
    "\n",
    "#initiate chromedriver, set download directory to TempDir1, make headless, get CAISO page.\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\" : TempDir1_path}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "if headless == 'yes':\n",
    "    options.add_argument('headless')\n",
    "browser = webdriver.Chrome(executable_path=chromedriver, options=options) #DeprecationWarning: use options instead of chrome_options // browser = webdriver.Chrome(executable_path=chromedriver, chrome_options=options)\n",
    "browser.get(url)\n",
    "\n",
    "#=================================================================================\n",
    "#DOWNLOADER\n",
    "\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "try:\n",
    "    element = wait.until(EC.element_to_be_clickable((By.XPATH, date_xpath)))\n",
    "except:\n",
    "    print('Could not find date field')\n",
    "'''\n",
    "for element in elements:\n",
    "    if 'batteries-date' in element.get_attribute('class'):       \n",
    "'''        \n",
    "#Enters date\n",
    "while date <= enddate:\n",
    "    element.click()\n",
    "    element.clear()\n",
    "    date_input = (date.strftime(\"%m/%d/%Y\"))\n",
    "    element.send_keys(date_input)\n",
    "    element.send_keys(Keys.ENTER)\n",
    "\n",
    "    #Clicks download link\n",
    "    button1 = browser.find_element(By.XPATH, dropdown_xpath)\n",
    "    time.sleep(1.5)       #make sure it has time to load. Otherwise, there can be missing values or duplicates. I TRIED CHANGING 1.5 TO 2, BUT IT DIDN'T REDUCE DUPLICATES.\n",
    "    button1.click()\n",
    "    button2 = browser.find_element(By.XPATH, download_xpath)\n",
    "\n",
    "    #in case the browser is lagging, it will keep trying to click the link\n",
    "    while True:\n",
    "        try:\n",
    "            button2.click()\n",
    "            print(\"downloading\", date_input)\n",
    "        except:     #add specific exceptions?\n",
    "            time.sleep(0.5)\n",
    "            print(\"needed a nap\")\n",
    "            continue\n",
    "        break\n",
    "    date = date + datetime.timedelta(1)\n",
    "\n",
    "#kill invisiable headless browser\n",
    "time.sleep(2)        #make sure the last date has time to download.\n",
    "browser.close()\n",
    "\n",
    "#=================================================================================\n",
    "#FORMAT DAILY DATA\n",
    "def autotrim(data, date):\n",
    "    print(f'autotrimming {date} because it has {len(data.columns)} values.')\n",
    "    if data.iloc[0,288] == '00:00' or data.iloc[0,288] == '0:00':\n",
    "        data.drop(data.columns[[288]], axis=1, inplace=True)\n",
    "        print(f'Fixed {date}. Now has {len(data.columns)} values.')\n",
    "    else:\n",
    "        print(f'{date} still 289 values. Check manually.')\n",
    "    return(data)\n",
    "\n",
    "#grabbing csv's from TempDir1, transposing, stripping headings, adding date column\n",
    "os.chdir(TempDir1_path)\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]            \n",
    "print(\"{} files in all_filenames\".format(len(all_filenames)))\n",
    "print(\"formatting data...\")\n",
    "missing_data_log = []\n",
    "\n",
    "for f in range(0,len(all_filenames)):\n",
    "    data = pd.read_csv(TempDir1_path+all_filenames[f], header=None, index_col=0)\n",
    "    \n",
    "    #Cell A1 (data[0][0]) contains the date, and sometimes other text\n",
    "    date_raw = (data.iloc[0].name)\n",
    "    #Extract the date from the above string by finding the positions of the first and last digits in the string.\n",
    "    date_pos = [i for i,c in enumerate(date_raw) if c.isdigit()]\n",
    "    date = date_raw[min(date_pos):max(date_pos)+1]\n",
    "    print(\"formatting\", date)\n",
    "    \n",
    "    #fix the double-0:00 issue\n",
    "    if len(data.columns) == 289:\n",
    "        autotrim(data, date)\n",
    "\n",
    "    #transpose & creating date column\n",
    "    df = data.T\n",
    "    date_column = [date]*len(df)\n",
    "  \n",
    "    #flag dates with missing or extra data\n",
    "    if len(df) != 288:\n",
    "        print(\"warning: {} has {} elements\".format(date,len(df)))\n",
    "        missing_data_log.append(date)\n",
    "    \n",
    "    #add date column & save csv to TempDir2\n",
    "    df.insert(0,\"Date\", date_column,True)\n",
    "    df.rename(columns={date_raw: 'Time'}, inplace = True)\n",
    "    df.to_csv(TempDir2_path+all_filenames[f])    \n",
    "\n",
    "    \n",
    "#=================================================================================\n",
    "#CONCATENATOR+SORTER\n",
    "\n",
    "os.chdir(TempDir2_path)\n",
    "day_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "#combine all files in the list\n",
    "print(\"concatenating...\")\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in day_filenames])\n",
    "\n",
    "#create datetime object for sorting\n",
    "print(\"combined_csv\")\n",
    "combined_csv.to_csv(TempDir2_path+'combined_csv.csv')\n",
    "print(\"saved combined csv\")\n",
    "\n",
    "#Add datetime column\n",
    "#I added Try/Except because one value in CAISO's import data was missing a timestamp, which caused an error when I tried to strip the time, but it was still able to make a datetime object without stripping.\n",
    "print(\"adding datetime for sorting...\")\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "date_error_counter = 0\n",
    "time_error_counter = 0\n",
    "for index,row in df.iterrows():\n",
    "    try:\n",
    "        DATE = df.loc[index,'Date'].strip()\n",
    "    except Exception as e:\n",
    "        date_message = str(e)\n",
    "        print (f\"Failed to strip Date for index: {index}, row: {row}. Error message: {date_message}\")\n",
    "        date_error_counter += 1\n",
    "    try:\n",
    "        TIME = df.loc[index,'Time'].strip()\n",
    "    except Exception as e:\n",
    "        time_message = str(e)\n",
    "        print (f\"Failed to strip Time for index: {index}, row: {row}. Error message: {time_message}\")\n",
    "        time_error_counter += 1\n",
    "    df.loc[index,'Datetime'] = datetime.datetime.combine(datetime.datetime.strptime(DATE, '%m/%d/%Y') ,datetime.datetime.strptime(TIME,'%H:%M').time())\n",
    "try:\n",
    "    print (f\"Failed to strip Date for {date_error_counter} rows. Error message: {date_message}\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    print (f\"Failed to strip Time for {time_error_counter} rows. Error message: {time_message}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#sort by datetime\n",
    "df = df.drop('Unnamed: 0.1', axis=1)\n",
    "print(\"sorting\")\n",
    "df = df.sort_values(by='Datetime')\n",
    "final_df = df.reset_index(drop=True)\n",
    "\n",
    "#=================================================================================\n",
    "#REPORT + SAVE\n",
    "\n",
    "#report missing data\n",
    "print(\"-------------------------\")\n",
    "print(\"dates queried:\", enddate - startdate + datetime.timedelta(1))\n",
    "print(\"dates returned:\", len(all_filenames))\n",
    "print(\"Given dates returned:\")\n",
    "print(\"  values expected:\", 288*len(all_filenames))\n",
    "print(\"  values returned:\", len(final_df))\n",
    "print(\"dates with missing or extra data:\", missing_data_log)\n",
    "print(\"-------------------------\")\n",
    "\n",
    "#save to Results folder\n",
    "final_df.to_csv(Results_path+filename)\n",
    "print(\"saved final csv\")\n",
    "\n",
    "\n",
    "#=================================================================================\n",
    "#DUPLICATE CHECK\n",
    "\n",
    "print(\"checking for duplicates...\")\n",
    "df = pd.read_csv(Results_path+filename)  ###Untested###\n",
    "df['Date'] = df['Date'].str.strip()\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "df.head()\n",
    "date = startdate\n",
    "duplicate_data_log = []\n",
    "previous = []\n",
    "notfirstday = 0\n",
    "\n",
    "#Use values in second-to-last column for Duplicate Check\n",
    "value_col = df.columns[-2]\n",
    "print(f'Value column used in Duplicate Check: {value_col}')\n",
    "\n",
    "while date <= enddate:\n",
    "    date_input = (date.strftime(\"%m/%d/%Y\"))\n",
    "    elements = df.loc[[date_input],[value_col]]   ###Untested###\n",
    "    if notfirstday == 0:\n",
    "        pass\n",
    "    elif np.array_equal(elements.values,previous.values):\n",
    "        print(\"warning: {} is a duplicate of the previous date\".format(date_input))\n",
    "        duplicate_data_log.append(date_input)\n",
    "    previous = elements\n",
    "    date = date + datetime.timedelta(1)\n",
    "    notfirstday = 1\n",
    "print(\"duplicate_data_log:\",duplicate_data_log)\n",
    "toc = time.time()\n",
    "print(toc-tic,'seconds elapsed')\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f667e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
